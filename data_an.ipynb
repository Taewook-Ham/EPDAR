{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = \"\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = [json.loads(line) for line in json_file]\n",
    "\n",
    "\n",
    "for item in data:\n",
    "    for positive_ctx in item[\"positive_ctxs\"]:\n",
    "        if positive_ctx.get(\"title\") == \"Generated\":\n",
    "            print(len(item[\"question\"]))\n",
    "            # print(\"Question:\", item[\"question\"])\n",
    "            # print(\"Answers:\", item[\"answers\"])\n",
    "            # print(\"Text:\", positive_ctx.get(\"text\"))\n",
    "            # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cases where \"title\" is \"Generated\" in positive_ctxs: 60480\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "json_file_path = ''\n",
    "\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "generated_count = 0\n",
    "for item in data:\n",
    "    positive_ctxs = item.get('positive_ctxs', [])\n",
    "    for ctx in positive_ctxs:\n",
    "        if ctx.get('title', '').lower() == 'generated':\n",
    "            generated_count += 1\n",
    "\n",
    "print(f'The number of cases where \"title\" is \"Generated\" in positive_ctxs: {generated_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = ''\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "for i, item in enumerate(data[:3]):\n",
    "    print(f\"[{i}]: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = \"\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = [json.loads(line) for line in json_file]\n",
    "\n",
    "\n",
    "generated_title_count = 0\n",
    "for item in data:\n",
    "    for positive_ctx in item[\"positive_ctxs\"]:\n",
    "        if positive_ctx.get(\"title\") == \"Generated\":\n",
    "            generated_title_count += 1\n",
    "\n",
    "print(\"Number of questions with 'Generated' title:\", generated_title_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = \"\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = [json.loads(line) for line in json_file]\n",
    "\n",
    "\n",
    "question_count = {}\n",
    "duplicate_questions = []\n",
    "\n",
    "for item in data:\n",
    "    question = item[\"question\"]\n",
    "    if question in question_count:\n",
    "        question_count[question] += 1\n",
    "        if question not in duplicate_questions:\n",
    "            duplicate_questions.append(question)\n",
    "    else:\n",
    "        question_count[question] = 1\n",
    "\n",
    "\n",
    "if duplicate_questions:\n",
    "    print(\"Duplicate questions found:\")\n",
    "    for question in duplicate_questions:\n",
    "        print(f\"Question: {question}, Count: {question_count[question]}\")\n",
    "else:\n",
    "    print(\"No duplicate questions found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = \"\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = [json.loads(line) for line in json_file]\n",
    "\n",
    "\n",
    "questions = [item[\"question\"] for item in data]\n",
    "\n",
    "\n",
    "unique_questions = set(questions)\n",
    "duplicate_count = len(questions) - len(unique_questions)\n",
    "\n",
    "\n",
    "print(f\"Total number of questions: {len(questions)}\")\n",
    "print(f\"Number of duplicate questions: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = \"\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    lines = json_file.readlines()\n",
    "\n",
    "\n",
    "unique_questions = set()\n",
    "\n",
    "\n",
    "unique_data = []\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    try:\n",
    "        json_data = json.loads(line)\n",
    "        question = json_data[\"question\"]\n",
    "        if question not in unique_questions:\n",
    "            unique_data.append(json_data)\n",
    "            unique_questions.add(question)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding error: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Total number of questions: {len(lines)}\")\n",
    "print(f\"Number of unique questions: {len(unique_questions)}\")\n",
    "print(f\"Length of unique_data: {len(unique_data)}\")\n",
    "\n",
    "\n",
    "with open(\"unique_output_data.json\", \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for data in unique_data:\n",
    "        json.dump(data, output_file, ensure_ascii=False)\n",
    "        output_file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON: 100%|██████████| 1/1 [00:32<00:00, 32.40s/ lines]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted data saved to: /home/htw/ipy/DAR/downloads/data/retriever/nq-train.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "input_file_path = \"\"\n",
    "output_file_path = \"\" \n",
    "\n",
    "\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "    lines = json_file.readlines()\n",
    "\n",
    "\n",
    "formatted_data_list = []\n",
    "for line in tqdm(lines, desc=\"Processing JSON\", unit=\" lines\"):  \n",
    "    try:\n",
    "        json_data = json.loads(line)\n",
    "        formatted_data_list.append(json_data)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decoding error: {e}\")\n",
    "\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    json.dump(formatted_data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "print(\"Formatted data saved to:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = ''\n",
    "\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for item in tqdm(data, desc=\"Processing Data\"):\n",
    "    positive_ctxs = item.get(\"positive_ctxs\", [])\n",
    "    if positive_ctxs:\n",
    "        first_positive_text = positive_ctxs[0].get(\"text\", \"\")\n",
    "        for ctx in positive_ctxs:\n",
    "            if ctx.get(\"title\", \"\") == \"Generated\":\n",
    "                generated_text = ctx.get(\"text\", \"\")\n",
    "                first_positive_text += generated_text\n",
    "        positive_ctxs[0][\"text\"] = first_positive_text\n",
    "    combined_data.append(item)\n",
    "\n",
    "output_file_path = 'modified_data.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(combined_data, output_file, ensure_ascii=False)\n",
    "\n",
    "for i, item in enumerate(combined_data[:3]):\n",
    "    print(f\"[{i}]: {item}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = ''\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for item in tqdm(data, desc=\"Processing Data\"):\n",
    "    positive_ctxs = item.get(\"positive_ctxs\", [])\n",
    "    if positive_ctxs:\n",
    "        for ctx in positive_ctxs:\n",
    "            if \"text\" in ctx:\n",
    "                ctx[\"text\"] = ctx[\"text\"].replace(\"\\n\", \"\")\n",
    "    combined_data.append(item)\n",
    "\n",
    "output_file_path = 'modified_data.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(combined_data, output_file, ensure_ascii=False)\n",
    "\n",
    "for i, item in enumerate(combined_data[:3]):\n",
    "    print(f\"[{i}]: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = ''\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for item in tqdm(data, desc=\"Processing Data\"):\n",
    "    data_dict = {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answers\": item[\"answers\"],\n",
    "        \"positive_ctxs\": [],\n",
    "        \"negative_ctxs\": item[\"negative_ctxs\"],\n",
    "        \"hard_negative_ctxs\": item[\"hard_negative_ctxs\"]\n",
    "    }\n",
    "\n",
    "    for ctx in item[\"positive_ctxs\"]:\n",
    "        data_dict[\"positive_ctxs\"].append({\n",
    "            \"title\": ctx[\"title\"],\n",
    "            \"text\": ctx[\"text\"]\n",
    "        })\n",
    "\n",
    "    data_list.append(data_dict)\n",
    "\n",
    "output_file_path = ''\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(data_list, output_file, ensure_ascii=False)\n",
    "\n",
    "for i, item in enumerate(data_list[:3]):\n",
    "    print(f\"[{i}]: {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = ''\n",
    "\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for item in data:\n",
    "    question = item[\"question\"]\n",
    "    answers = item[\"answers\"]\n",
    "    positive_ctxs = item[\"positive_ctxs\"]\n",
    "    \n",
    "    generated_ctxs = [ctx for ctx in positive_ctxs if ctx[\"title\"] == \"Generated\"]\n",
    "    \n",
    "    if generated_ctxs:\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Answers:\", answers)\n",
    "        print(\"Positive Contexts 0th Title:\", positive_ctxs[0][\"title\"])\n",
    "        print(\"Positive Contexts 0th Text:\", positive_ctxs[0][\"text\"])\n",
    "        print(\"Generated Context Title:\", generated_ctxs[0][\"title\"])\n",
    "        print(\"Generated Context Text:\", generated_ctxs[0][\"text\"])\n",
    "        print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "global conut\n",
    "\n",
    "count =0\n",
    "\n",
    "with open('', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for item in data:\n",
    "    found = False\n",
    "    for context in item[\"positive_ctxs\"]:\n",
    "        if context[\"title\"] == \"Generated\":\n",
    "            found = True\n",
    "            text = context[\"text\"].lower().replace(\" \", \"\")  \n",
    "            text1 = context[\"text\"].lower()\n",
    "            missing_answers = []\n",
    "            for answer in item[\"answers\"]:\n",
    "                if answer.lower().replace(\" \", \"\") not in text:\n",
    "                    missing_answers.append(answer)\n",
    "\n",
    "            if missing_answers:\n",
    "                print()\n",
    "                print(f\"Warning: The following answers are not present in the context text for question: '{item['question']}'\")\n",
    "                for answer in missing_answers:\n",
    "                    print(f\"- Missing answer: '{answer}'\")\n",
    "                print(f\"Related Text: {text1}\") \n",
    "                print()\n",
    "                count += 1\n",
    "\n",
    "            break \n",
    "\n",
    "    if not found:\n",
    "        print(f\"Warning: 'Generated' title not found for question: '{item['question']}'\")\n",
    "\n",
    "print(\"count = \", count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Length: 58876\n",
      "Filtered Data saved to /home/htw/ipy/DAR/downloads/data/retriever/filtered_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for item in data:\n",
    "    found = False\n",
    "    for context in item.get(\"positive_ctxs\", []):\n",
    "        if context.get(\"title\", \"\") == \"Generated\":\n",
    "            found = True\n",
    "            text = context.get(\"text\", \"\").lower().replace(\" \", \"\")  \n",
    "            missing_answers = []\n",
    "            for answer in item.get(\"answers\", []):\n",
    "                if answer.lower().replace(\" \", \"\") not in text:\n",
    "                    missing_answers.append(answer)\n",
    "\n",
    "            if not missing_answers:\n",
    "                filtered_data.append(item)\n",
    "                break  \n",
    "            else:\n",
    "                item2 = item.copy()\n",
    "                positive_ctxs = item2.get(\"positive_ctxs\", [])\n",
    "                item2[\"positive_ctxs\"] = [ctx for ctx in positive_ctxs if ctx.get(\"title\", \"\") != \"Generated\"]\n",
    "                filtered_data.append(item2)\n",
    "\n",
    "\n",
    "    if not found:\n",
    "\n",
    "        filtered_data.append(item)\n",
    "\n",
    "\n",
    "output_filename = ''\n",
    "with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(filtered_data, output_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Filtered Data Length: {len(filtered_data)}\")\n",
    "print(f\"Filtered Data saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of questions with title 'Generated': 58876\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('', 'r', encoding='utf-8') as file:\n",
    "    filtered_data = json.load(file)\n",
    "\n",
    "\n",
    "generated_question_count = sum(1 for item in filtered_data if any(ctx.get(\"title\", \"\") == \"Generated\" for ctx in item.get(\"positive_ctxs\", [])))\n",
    "\n",
    "print(f\"The number of questions with title 'Generated': {generated_question_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity Score: 0.01848292553241764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "with open('', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "texts = []\n",
    "questions = []\n",
    "\n",
    "\n",
    "for item in data:\n",
    "    positive_ctxs = item[\"positive_ctxs\"]\n",
    "    if positive_ctxs:\n",
    "        texts.append(positive_ctxs[0][\"text\"])\n",
    "        questions.append(item[\"question\"])\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "\n",
    "total_similarity_score = 0\n",
    "for question in questions:\n",
    "\n",
    "    question_tfidf = vectorizer.transform([question])\n",
    "\n",
    "\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix, question_tfidf)\n",
    "\n",
    "\n",
    "    average_similarity_score = np.mean(similarity_scores)\n",
    "    total_similarity_score += average_similarity_score\n",
    "\n",
    "\n",
    "average_similarity_score = total_similarity_score / len(questions)\n",
    "print(\"Average Cosine Similarity Score:\", average_similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Similarity: 100%|██████████| 58876/58876 [1:00:49<00:00, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity Score with Generated Contexts: 0.03191999083330919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with open('', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "questions = []\n",
    "generated_texts = []\n",
    "\n",
    "\n",
    "for item in data:\n",
    "    question = item[\"question\"]\n",
    "    questions.append(question)\n",
    "    positive_ctxs = item[\"positive_ctxs\"]\n",
    "    if positive_ctxs:\n",
    "        for ctx in positive_ctxs:\n",
    "            if ctx[\"title\"] == \"Generated\":\n",
    "                generated_texts.append(ctx[\"text\"])\n",
    "                break  \n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_questions = vectorizer.fit_transform(questions)\n",
    "tfidf_matrix_generated_texts = vectorizer.transform(generated_texts)\n",
    "\n",
    "\n",
    "total_similarity_score = 0\n",
    "with tqdm(total=len(questions), desc=\"Calculating Similarity\") as pbar:\n",
    "    for i, question in enumerate(questions):\n",
    "\n",
    "        question_tfidf = tfidf_matrix_questions[i]\n",
    "\n",
    "\n",
    "        similarity_scores_generated_texts = cosine_similarity(question_tfidf, tfidf_matrix_generated_texts)\n",
    "\n",
    "\n",
    "        average_similarity_score_generated_texts = np.mean(similarity_scores_generated_texts)\n",
    "        total_similarity_score += average_similarity_score_generated_texts\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "average_similarity_score = total_similarity_score / len(questions)\n",
    "print(\"Average Cosine Similarity Score with Generated Contexts:\", average_similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity Score: 0.01632367160477369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "with open('', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "texts = []\n",
    "questions = []\n",
    "\n",
    "for item in data:\n",
    "    negative_ctxs = item[\"negative_ctxs\"]\n",
    "    if negative_ctxs:\n",
    "        texts.append(negative_ctxs[0][\"text\"])\n",
    "        questions.append(item[\"question\"])\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "\n",
    "total_similarity_score = 0\n",
    "for question in questions:\n",
    "\n",
    "    question_tfidf = vectorizer.transform([question])\n",
    "\n",
    "\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix, question_tfidf)\n",
    "\n",
    "\n",
    "    average_similarity_score = np.mean(similarity_scores)\n",
    "    total_similarity_score += average_similarity_score\n",
    "\n",
    "\n",
    "average_similarity_score = total_similarity_score / len(questions)\n",
    "print(\"Average Cosine Similarity Score:\", average_similarity_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
